---
title: "Equations"
output:
  word_document: default
  html_document: default
date: "2024-05-15"
---

This Markdown is used exclusively for any equations required for the dissertation writing

## 1D

Moffat Model:
$$f(x) = A(1+\frac{(x-\mu)^2}{\alpha^2})^{-\beta}$$
Where A = amplitude, $\mu$ = position of the maximum, $\alpha$ = core width, $\beta$ = power index.

Moffat Distribution:
$$f(x) = \frac{\Gamma(\beta)}{\alpha\sqrt\pi~\Gamma(\beta-\frac{1}{2})} \left[1+\frac{(x-\mu)^2}{\alpha^2}\right]^{-\beta}$$
To obtain the Negative Log Likelihood.
First I take the Likelihood of the parameters of f(x). 
$$L(x; \mu, \alpha, \beta) = \prod f(x)$$
Next I take the log of the likelihood and apply the log law
$ln\left[ab\right] = ln\left[a\right]+ln\left[b\right]$.

$$ln\left[L(x; \mu, \alpha, \beta)\right] = \sum(ln\left[f(x)\right])$$
Taking the negative of this yields the Negative Log Likelihood.
Let NLL = Negative Log Likelihood

$$NLL = -\sum\ln\left[\frac{\Gamma(\beta)}{\alpha\sqrt\pi~\Gamma(\beta-\frac{1}{2})} \left[1+\frac{(x-\mu)^2}{\alpha^2}\right]^{-\beta}\right]$$

By applying more log laws, this can be expanded.
Let b = $\Gamma(\beta)$
Let c = $\alpha\sqrt\pi~\Gamma(\beta-\frac{1}{2})$
Let d = $\left[1+\frac{(x-\mu)^2}{\alpha^2}\right]$

$$ NLL =  -\sum\ln\left[\frac{b}{c}d^{-\beta}\right]$$

$ln\left[\frac{b}{c}d^{-\beta}\right] = ln\left[\frac{b}{c}\right]+ln\left[d^{-\beta}\right]$.
$ln\left[\frac{b}{c}\right] = ln\left[b\right]-ln\left[c\right]$.
$ln\left[d\right]^{-\beta} = -\beta\ln\left[a\right]$.

$$ = -\sum( \ln\left[b\right] - \ln\left[c\right] + \ln\left[d^{-\beta}])$$
c can be split into two parts
Let k = $\alpha\sqrt\pi$
Let l = $\Gamma(\beta-\frac{1}{2})$

$$ = -\sum( \ln\left[b\right] - (\ln\left[k\right] + \ln\left[l\right]) -\beta\ln\left[d])$$
Substituting the parameters back in.

$$NLL = -\sum(\ln\left[\Gamma(\beta)\right] - (\ln\left[\alpha\sqrt\pi\right] + \ln\left[\Gamma(\beta-\frac{1}{2})\right]) - \beta\ln\left[1+\frac{(x-\mu)^2}{\alpha^2}\right])$$
In this form it is easier to find the partial derivatives.


In order to take the partial derivative with respect to $\mu$ I utilised the chain rule.
$\frac{\partial}{\partial\mu} = \frac{\partial}{\partial u} \frac{\partial u}{\partial v} \frac{\partial v}{\partial \mu}$

Let u = $\left[1+\frac{(x-\mu)^2}{\alpha^2}\right]$

Let v = $(x - \mu)$
$\frac{\partial v}{\partial \mu} = -1$

u = $\left[1+\frac{v^2}{\alpha^2}\right]$
$\frac{\partial u}{\partial v}  = 2v\alpha^{-2} = \frac{2(x - \mu)}{\alpha^{-2}}$

NLL = $\ln\left[\Gamma(\beta)\right] - (\ln\left[\alpha\sqrt\pi\right] + \ln\left[\Gamma(\beta-\frac{1}{2})\right]) - \beta\ln\left[u \right]$ 

$\frac{\partial }{\partial u} = -\sum-\frac{\beta}{u} = -\sum\left[-\frac{\beta}{1+\frac{v^2}{\alpha^2}}\right]$

Resulting in

$$\frac{\partial}{\partial\mu} = -\sum(-\frac{\beta}{1+\frac{(x - \mu)^2}{\alpha^2}} \frac{2(x - \mu)}{\alpha^2}\times{-1})$$
$$\frac{\partial}{\partial\mu} = -\sum\frac{2\beta(x-\mu)}{\alpha^2 + (x-\mu)^2}$$


To take the partial derivative with respect to $\alpha$ it is slightly more complex, as $\alpha$ exists twice within the Negative Log Likelihood. However, we can find the derivative to the two parts seperately.

Let u = $\left[1+\frac{(x-\mu)^2}{\alpha^2}\right]$

$\ln\left[\Gamma(\beta)\right] - (\ln\left[\alpha\sqrt\pi\right] + \ln\left[\Gamma(\beta-\frac{1}{2})\right]) - \beta\ln\left[u \right]$

Taking out $(\ln\left[\alpha\sqrt\pi\right] + \ln\left[\Gamma(\beta-\frac{1}{2})\right])$
The partial derivative with respect to $\alpha$ here is 
$\frac{1}{-\alpha}$.

For $-\beta\ln\left[u\right]$
u = $\left[1+(x-\mu)^2\alpha^{-2}\right]$
$\frac{\partial u}{\partial \alpha}  = 2(x - \mu)^2\alpha^{-3}$

$\frac{\partial}{\partial u}  = -\frac{\beta}{u} = \frac{\beta}{\left[1+(x-\mu)^2\alpha^{-2}\right]}$


Combined this becomes
$$\frac{\partial}{\partial\alpha} = -\sum(-\frac{1}{\alpha} + \frac{\beta}{\left[1+(x-\mu)^2\alpha^{-2}\right]}\times\frac{2(x - \mu)^2}{\alpha^{3}}$$

$$\frac{\partial}{\partial\alpha} = -\sum(-\frac{1}{\alpha} + \frac{2\beta(x-\mu)^2}{\alpha^3 + \alpha(x-\mu)^2})$$


To obtain the partial derivative with respect to beta, make use of the digamma function in R.
$\frac{\partial}{\partial\beta} \ln\left[\Gamma(\beta)\right] = \frac{\Gamma'(\beta)}{\Gamma(\beta)}$ = digamma($\beta$)

$$\frac{\partial}{\partial\beta} = -\sum (digamma(\beta) - digamma(\beta - \frac{1}{2}) - \ln\left[1+\frac{(x-mu)^2}{alpha^2}\right])$$

## 2D 
The 2D Moffat Model has this form:

$$f(x,y) = A\left[1 + \frac{(x-\bar{x_0})^2 + (y - \bar{y_0})^2}
{\alpha^2}\right]^{-\beta} $$

Where A = amplitude, $\bar{x_0}$ = x-position of the maximum, $\bar{y_0}$ = y-position of the maximum, $\alpha$ = core width, $\beta$ = power index.

This is a symmetric uncorrelated function.

Similarly to the 1D Moffat, the amplitude must be greater than 0. Alpha can't be equal to zero and Beta must be greater than 0.

$$-\sum(\ln\left[A\right]-\beta\ln\left[1+\frac{(x-\bar{x_0})^2 + (y-\bar{y_0})^2}
{\alpha^2}\right]) $$


This two must be converted to the moffat distribution:

Moffat Distribution:
$$f(x,y) = \frac{\beta-1}{\pi\alpha^2}\left[1 + \frac{(x-\bar{x_0})^2 + (y - \bar{\bar{y_0}})^2}
{\alpha^2}\right]^{-\beta} $$

Gradient of the negative log likelihood with respect to each parameter
$$\frac{\partial}{\partial \bar{x_0}} = -\sum\frac{2\beta(x-\bar{x_0})}{\alpha^2+(x-\bar{x_0})^2+(y-\bar{y_0})^2}$$

$$\frac{\partial}{\partial \bar{y_0}} = -\sum\frac{2\beta(y-\bar{y_0})}{\alpha^2+(x-\bar{x_0})^2+(y-\bar{y_0})^2}$$


$$ \frac{\partial}{\partial\alpha} = 2\sum(\frac{1}{\alpha} + \frac{\beta((x-\bar{x_0})^2 + (y-\bar{y_0})^2)}{\alpha^3 + \alpha((x-\bar{x_0})^2+(y-\bar{y_0})^2)})$$


$$ \frac{\partial}{\partial\beta} = -\sum(\frac{1}{\beta} - \ln\left[1+\frac{(x-\bar{x_0})^2 + (y-\bar{y_0})^2}{\alpha^2}\right])$$



## Spatial Mixture Models

Creating a mixture model that uses two 2D moffat functions. 
$$ f(x_j, y_j) = \sum^{2}_{i=1} \lambda_i f_i(x_j, y_j)$$
Where $\lambda_i$ is the mixing proportion parameter and $f_i(x_j, y_j)$ are the component densities. Which in the case of a Moffat density looks like this.
$$f_i(x_j, y_j) = \frac{\beta_i-1}{\pi\alpha_i^2}\left[1 + \frac{(x_j-\bar{x}_{i})^2 + (y_j - \bar{y}_{i})^2}{\alpha_i^2}\right]^{-\beta_i} $$
The mixture model in full:
$$ f(x_j, y_j) = \lambda_1\frac{\beta_1-1}{\pi\alpha_1^2}\left[1 + \frac{(x_j-\bar{x}_{1})^2 + (y_j - \bar{y}_{1})^2}{\alpha_1^2}\right]^{-\beta_1} + \lambda_2\frac{\beta_2-1}{\pi\alpha_2^2}\left[1 + \frac{(x_j-\bar{x}_{2})^2 + (y_j - \bar{y}_{2})^2}{\alpha_2^2}\right]^{-\beta_2}$$
Let $v_i$ = $1 + \frac{(x_j-\bar{x}_{i})^2 + (y_j - \bar{y}_{i})^2}{\alpha_i^2}$
Let $c_i$ = $\frac{(\beta_i-1)}{\pi\alpha_{i}^{2}}$

$$f(x_j, y_j) = \lambda_1 c_1 v_1^{-\beta_1} + \lambda_2 c_2 v_2^{-\beta_2}$$
The Negative Log Likelihood:
$$-\sum^{n}_{j=1}\ln\left[\lambda_1 c_1 v_1^{-\beta_1} + \lambda_2 c_2 v_2^{-\beta_2}\right]$$
To find the partial derivatives of the Negative Log Likelihood
Throughout the derivations the chain rule, quotient rule and product rule are utilised
Chain Rule: $\frac{\partial}{\partial x} = \frac{\partial}{\partial y} \frac{\partial y}{\partial x}$
Quotient Rule: $\frac{\partial}{\partial x} = \frac{ v \frac{\partial u}{\partial x} - { u \frac{\partial v}{\partial x}}}{v^2}$
Product Rule:  $\frac{\partial (uv)}{\partial x} = u \frac{\partial v}{\partial x} + { v \frac{\partial u}{\partial x}}$

Let:
$$u = \lambda_1 c_1 v_1^{-\beta_1} + \lambda_2 c_2 v_2^{-\beta_2}$$

For $\frac{\partial}{\partial x_1}$:

$$\frac{\partial}{\partial x_1} = \frac{\partial}{\partial u} \frac{\partial u}{\partial v_1} \frac{\partial v_1}{\partial x_1}$$
$$\frac{\partial}{\partial u} = -\sum\frac{1}{u}$$
$$\frac{\partial u}{\partial v_1} = - \lambda_1 c_1 \beta_1 v_{1}^{-(\beta_1 + 1)}$$
$$\frac{\partial v_1}{\partial x_1} = \frac{-2(x_j - \bar{x}_1)}{\alpha_{1}^{2}}$$
$$\frac{\partial}{\partial x_1} = -\sum\frac{1}{u} (-\lambda_1 c_1 \beta_1 v_{1}^{-(\beta_1 + 1)})(\frac{-2(x_j - \bar{x}_1)}{\alpha_{1}^{2}})$$
Simplifying to:
$$\frac{\partial}{\partial x_1} = -\sum\frac{2\lambda_1 c_1 \beta_1(x_j - \bar{x}_1)}{u  v_{1}^{\beta_1 + 1} \alpha_{1}^{2}}$$

A similar method is used to find the partial derivative with respect to $\bar{x}_2$, $\bar{y}_1$ and $\bar{y}_2$.

$$\frac{\partial}{\partial x_2} = -\sum\frac{2\lambda_2 c_2 \beta_2(x_j - \bar{x}_2)}{u  v_{2}^{\beta_2 + 1} \alpha_{2}^{2}}$$
$$\frac{\partial}{\partial y_1} = -\sum\frac{2\lambda_1 c_1 \beta_1(y_j - \bar{y}_1)}{u  v_{1}^{\beta_1 + 1} \alpha_{1}^{2}}$$

$$\frac{\partial}{\partial y_2} = -\sum\frac{2\lambda_2 c_2 \beta_2(y_j - \bar{y}_2)}{u  v_{2}^{\beta_2 + 1} \alpha_{2}^{2}}$$
For the partial derivative with respect to $\lambda_1$.
$$\frac{\partial u}{\partial \lambda_1} = c_1  v_{1}^{-\beta_1}$$
$$\frac{\partial}{\partial \lambda_1} = -\sum\frac{c_1 v_{1}^{-\beta_1}}{u}$$
Similarly for $\lambda_2$
$$\frac{\partial}{\partial \lambda_2} = -\sum\frac{c_2 v_{2}^{-\beta_2}}{u}$$

For the partial derivative with respect to $\alpha_1$
Let $w_1 = \frac{\lambda_1(\beta_1 -1)}{\pi}\alpha_{1}^{-2}$
Let $d_1 = v_1^{-\beta_1}$
So $u = wd$
Apply the product rule.

$w_1' = -2\frac{\lambda_1(\beta_1 - 1)}{\pi}\alpha_{1}^{-3}$

To find d' the quotient and chain rules are applied
$$d_1' = \frac{2 \beta_1\left[(x-x_1)^{2}+(y-y_1)^{2}\right]}{v_1^{\beta_1 +1}\alpha_1^3}$$

This can be simplified:
$$d_1' = \frac{2 \beta_1}{v_1^{\beta_1 +1}\alpha_1} \frac{\left[(x-x_1)^{2}+(y-y_1)^{2}\right]}{\alpha_1^2}$$
$$ = \frac{2 \beta_1}{v_1^{\beta_1} v_1 \alpha_1} v_1$$
$$ = \frac{2 \beta_1}{v_1^{\beta_1}\alpha_1}$$
Leading to:
$$\frac{\partial u}{\partial \alpha_1} = \frac{\lambda_1(\beta_1 -1)}{\pi\alpha_{1}^{2}} \frac{2 \beta_1}{v_1^{\beta_1}\alpha_1} + \frac{2 \lambda_1(\beta_1 - 1)}{\pi\alpha_{1}^{3}} \frac{1}{v_1^{\beta_1}}$$
This can be simplified:
$$\frac{\partial u}{\partial \alpha_1} = \frac{2 \lambda_1 \beta_1 (\beta_1 -1)}{\pi v_1^{\beta_1} \alpha_{1}^{3}} + \frac{2 \lambda_1(\beta_1 - 1)}{\pi v_1^{\beta_1} \alpha_{1}^{3}} $$
$$ = \frac{2 \lambda_1(\beta_1 - 1)}{\pi v_1^{\beta_1} \alpha_{1}^{3}} (\beta_1 + 1)$$
$$ = \frac{2 \lambda_1(\beta_1^2 - 1)}{\pi v_1^{\beta_1} \alpha_{1}^{3}}$$

Resulting in:
$$\frac{\partial}{\partial \alpha_1} = -\sum \frac{2 \lambda_1(\beta_1^2 - 1)}{\pi v_1^{\beta_1} \alpha_{1}^{3} u}$$

Similarly for $\alpha_2$
$$\frac{\partial}{\partial \alpha_2} = -\sum \frac{2 \lambda_2(\beta_2^2 - 1)}{\pi v_2^{\beta_2} \alpha_{2}^{3} u}$$

For the partial derivative with respect to $\beta_1$.
Rearrange the initial equation and
Let $s_i = \frac{\lambda_i}{\pi\alpha_{i}^{2}}$

$u = sv_{1}^{-\beta_1}(\beta_1 - 1)$
$$ u = sv_{1}^{-\beta_1}\beta_1 - sv_{1}^{-\beta_1}$$

The Product rule is employed to find the derivative of the first term.
$$\frac{\partial u}{\partial \beta_1} = -s_1 \beta_1 v_{1}^{-\beta_1} \ln(v_1) + s_1 v_1^{-\beta_1} + s_1 v_1^{-\beta_1}\ln(v_1)$$
$$\frac{\partial u}{\partial \beta_1} = s_1 v_1^{-\beta_1} (1 + \ln(v_1) - \beta_1 \ln(v_1))$$

$$\frac{\partial}{\partial \beta_1} = -\sum \frac{s_1 v_1^{-\beta_1} (1 + \ln(v_1) - \beta_1 \ln(v_1))}{u}$$
Similarly for $\beta_2$
$$\frac{\partial}{\partial \beta_2} = -\sum \frac{s_2 v_2^{-\beta_2} (1 + \ln(v_2) - \beta_2 \ln(v_2))}{u}$$
