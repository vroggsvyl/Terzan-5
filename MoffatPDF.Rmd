---
title: "Moffat1D"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2024-03-21"
---

## Set Up
Libraries
```{r}
library(ggplot2)
library(plotly)
library(readr)
library(ggpointdensity)
library(viridis)
library(tidyr)
library(fitdistrplus)

## Color palette
cbPalette <- c("#56B4E9", "#E69F00", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```

Importing the data.
```{r}
library(here)
i_am("MoffatPDF.Rmd")

# laptop data from csv
Terzan5 <- readr::read_csv(here("Data", "Terzan 5 X-ray events.csv"), col_types = list(.default = readr::col_guess()), )
# head(Terzan5)

#fishbowl data from ODS
# library(readODS)
# Terzan5 <- read_ods(here("Raw Data.ods"), col_types = list(.default = readr::col_guess()), )
#removing extraneous columns
T5 <- data.frame(Terzan5$x, Terzan5$y)
colnames(T5) <- c('x','y')
```

Viewing the dataset via a density heatmap.
```{r}
modelTerzan5 <- ggplot(T5,aes(x,y)) + 
  geom_pointdensity(adjust = 0.05, size = 0.1, shape = "1") + 
  scale_color_viridis(direction = -1, option = "B", trans = "log", breaks = c(10, 100, 1000, 10000)) + 
  coord_fixed()

modelTerzan5
```
Darker areas indicate areas containing a high number of observations. These indicates the existance of a celestial source. The first aim is to try to fit a Moffat Model to each individual celestial source to predict. By then removing all of the celestial sources and their associated observations, if there is a higher level remaining this is an indication of interstellar gas remaining within the cluster. 

Before getting to that stage, I want to focus on two individual celestial sources that are isolated (unlikely to contain observations that may come from another nearby source).

Choosing two Points of Interest (POI)
```{r}
POI1 <- T5 %>%
  filter(x>4160 & x<4180 & y>4180 & y<4200)
POI2 <- T5 %>%
  filter(x>4105 & x< 4125 & y>4040 & y<4060)
```

Viewing one of those Points of Interest
```{r}
modelPOI1 <- ggplot(POI1, aes(x,y)) +
  geom_pointdensity(adjust = 0.05, size = 0.1, shape = "1") + 
  scale_color_viridis(direction = -1, option = "B", trans = "log", breaks = c(10, 100, 1000)) + 
  coord_fixed()

modelPOI2 <- ggplot(POI2, aes(x,y)) +
  geom_pointdensity(adjust = 0.05, size = 0.1, shape = "1") + 
  scale_color_viridis(direction = -1, option = "B", trans = "log", breaks = c(10, 100, 1000)) + 
  coord_fixed()

RAWPOI1 <- ggplot(POI1, aes(x,y)) +
  geom_point(size = 0.5) +
  coord_fixed()

RAWPOI2<- ggplot(POI2, aes(x,y)) +
  geom_point(size = 0.5) +
  coord_fixed()

modelPOI1
modelPOI2
RAWPOI1
RAWPOI2
```
It's important to note that while a lot of the surrounding area appears to be empty, it is full of background observations. It appears blank as observations are darker in colour and more prominent when there is a higher number of other observations close by.

Before fitting a 2D Moffat Model, I want to simplify and fit a 1D model to see if it is the most appropriate fit to the data. To do that, I'm going to slice my celestial sources (POI's) into lines, with fewer observations and treat them as being one-dimensional. 
```{r}
# line0 <- POI2 %>%
#   filter(y>4048.5 & y< 4049.5)
# line1 <- POI2 %>%
#   filter(y>4049.5 & y< 4050.5)
line2 <- POI2 %>%
  filter(y>4050.5 & y< 4051.5)
# line3 <- POI2 %>%
#   filter(y>4051.5 & y< 4052.5)
# line4 <- POI2 %>%
#   filter(y>4052.5 & y< 4053.5)
# line5 <- POI2 %>%
#   filter(y>4053.5 & y< 4054.5)
# line6 <- POI2 %>%
#   filter(y>4054.5 & y< 4055.5)
# lineback <- POI2 %>%
#   filter(y>4055 & y< 4060)

line2.5 <- POI2 %>%
  filter(y>4050.5 & y< 4051)
```

```{r}
modelPOI2 + 
  geom_hline(yintercept = 4049.5) +
  geom_hline(yintercept = 4050.5) + 
  geom_hline(yintercept = 4051.5) + 
  geom_hline(yintercept = 4052.5) +
  geom_hline(yintercept = 4053.5) +
  geom_hline(yintercept = 4054.5)

RAWPOI2 +
  geom_hline(yintercept = 4049.5) +
  geom_hline(yintercept = 4050.5) + 
  geom_hline(yintercept = 4051.5) + 
  geom_hline(yintercept = 4052.5) +
  geom_hline(yintercept = 4053.5) +
  geom_hline(yintercept = 4054.5)

RAWPOI2 +
  geom_hline(yintercept = 4049.5) +
  geom_hline(yintercept = 4050.5, colour = "red") + 
  geom_hline(yintercept = 4051.5, colour = "red") + 
  geom_hline(yintercept = 4052.5) +
  geom_hline(yintercept = 4053.5) +
  geom_hline(yintercept = 4054.5)
```
## 1D Distribution Fitting
Here is a histogram of the data that I would like to try and model.
```{r}
histLine2 <- ggplot(line2) +
  geom_histogram(aes(x, y = after_stat(density), colour = "data"), bins = 50, fill=cbPalette[5])

histLine2
```

# Fitting to a Normal Distribution
```{r}
xl2 <- line2$x

# Define the Gaussian density function
gaussian_density <- function(parameters, x) {
  mean <- parameters[[1]]
  sd <- parameters[[2]]
  
  return(dnorm(x, mean = mean, sd = sd))
}

# Define the negative log-likelihood function for Gaussian distribution
negative_log_likelihood <- function(parameters, x) {
  predicted_density <- gaussian_density(parameters, x)
  return(-sum(log(predicted_density)))
}

initial_guess <- c(mean = 4115, sd = 1)

# Optimize the negative log-likelihood function
result <- optim(par = initial_guess, fn = negative_log_likelihood, x = xl2, method = "Nelder-Mead")

#check for convergence
result

# Extract the optimized parameters
optimized_parameters <- result$par
optimized_parameters
```
Overlay the plot of the normal distribution with the optimised parameters on the histogram of the data.
```{r}
optimParameters2N <- gaussian_density(optimized_parameters, xl2)

histWithNormL2 <- histLine2 +
  stat_function(fun = gaussian_density, args = list(parameters = optimized_parameters), n=1000, aes(colour="gaussian distribution"), linewidth=1.2) +
  ylab("p(x)") + 
  scale_colour_manual("Legend", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("right"),
    legend.box.just = "right",
    legend.margin = margin(6, 6, 6, 6))

histWithNormL2
```
It is clear that the Normal distribution doesn't provide an accurate model of the data.

I want to have a look at the 1D Moffat distribution. The Moffat Model was specifically designed to handle point spread functions that occur in the field of astronomy. So I expect to to yield a better fit to data than a normal distribution.

The 1D Moffat Model:
$$A(1+\frac{(x-\mu)^2}{\gamma^2})^{-\alpha}$$
Where A = amplitude, $\mu$ = position of the maximum, $\gamma$ = core width, $\alpha$ = power index.

Visualising the Moffat Function
```{r}
Moffat <- function(x, amplitude, mu, alpha, beta){
   amplitude*(1+((x-mu)^2/alpha^2))^(-beta)
}

#Function 1
A1 <- 4
mu1 <- 10
alpha1 <- 2
beta1 <- 3.8
String1 = paste("A =", A1, "mu =", mu1, "alpha =", alpha1, "beta =", beta1)

#Function 2
A2 <- 6
mu2 <- -3
alpha2 <- 0.7
beta2 <- 9
String2 = paste("A =", A2, "mu =", mu2, "alpha =", alpha2, "beta =", beta2)

#Function 3
A3 <- 2
mu3 <- 3.4
alpha3 <- 6.4
beta3 <- 0.7
String3 = paste("A =", A3, "mu =", mu3, "alpha =", alpha3, "beta = ", beta3)


ggplot() +
  stat_function(fun = Moffat, n = 1000, args = list(amplitude = A1, mu = mu1, alpha = alpha1, beta = beta1), aes(colour = String1)) +
  stat_function(fun = Moffat, n = 1000, args = list(amplitude = A2, mu = mu2, alpha = alpha2, beta = beta2), aes(colour = String2)) +
  stat_function(fun = Moffat, n = 1000, args = list(amplitude = A3, mu = mu3, alpha = alpha3, beta = beta3), aes(colour = String3)) +
  xlim(-20, 20) + ylim(0, NA) +
  scale_colour_manual("Moffat Models", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("left"),
    legend.margin = margin(6, 6, 6, 6))

```
Trying to use the optim function to estimate the parameters of the Moffat Model when fitted to the data.
```{r}
Moffat <- function(parameters, x){
  amplitude <- parameters[[1]]
  mu <- parameters[[2]]
  gamma <- parameters[[3]]
  alpha <- parameters[[4]]

  amplitude <- pmax(amplitude, 1e-6)
  gamma <- pmax(gamma, 1e-6)
  alpha <- pmax(alpha, 1e-6)
  #limiting values, amplitude must be greater than 0. As I am creating a probability density function, the area under the curve sums to 1. 
  #gamma can't equal zero, otherwise there is a math error.
  #alpha can't equal 0 otherwise the density will be equal to 1 for everything. It can't be less than zero as that'll cause the density to be greater than 1.

  predictedDensity <- amplitude*(1+((x-mu)^2/gamma^2))^(-alpha)
}

negLogLike <- function(parameters, x){
  predictedDens <- Moffat(parameters, x)
  return(-sum(log(predictedDens)))
}
```

When putting the above model into optim, the returned results for the list of parameters were unrealistic. In order to work out why I plotted the Negative Log Likelihood Function to varying parameters as the objective of the optim function is to minimize the Negative Log Likelihood Function.

The baseline parameters are (0.5, 0, 1, 1) and x is 1 in all estimates of the negative log likelihood.
```{r}
#changing the value of the amplitude
amplitude <- seq(0.25, 5, by = 0.25)

i <- 1
changingA <- vector("numeric", length = 20)

while(i<=20){
  parameters <- c(amplitude[[i]], 4115, 1, 1)
  changingA[i] <- (negLogLike(parameters, xl2))
  i <- i+1
}

#changing the value of mu
mu <- seq(4110, 4120, by = 0.5)

i <- 1
changingMu <- vector("numeric", length = 21)

while(i<=21){
  parameters <- c(0.5, mu[[i]], 1, 1)
  changingMu[i] <- (negLogLike(parameters, xl2))
  i <- i+1
}

#changing the value of gamma
gamma <- seq(0.25, 10, by = 0.25)

i <- 1
changingGamma <- vector("numeric", length = 40)

while(i<=40){
  parameters <- c(0.5, 4115, gamma[[i]], 1)
  changingGamma[i] <- (negLogLike(parameters, xl2))
  i <- i+1
}

#changing the value of alpha
alpha <- seq(0.25, 10, by = 0.25)

i <- 1
changingAlpha <- vector("numeric", length = 40)

while(i<=40){
  parameters <- c(0.5, 4115, 1, alpha[[i]])
  changingAlpha[i] <- (negLogLike(parameters, xl2))
  i <- i+1
}

par(mfrow = c(2,2))
plot(amplitude, changingA, type = "l", xlab = "Amplitude", ylab = "Negative Log Likelihood")
plot(mu, changingMu, type = "l", xlab = "Mu", ylab = "Negative Log Likelihood")
plot(gamma, changingGamma, type = "l", xlab = "Gamma", ylab = "Negative Log Likelihood")
plot(alpha, changingAlpha, type = "l", xlab = "Alpha", ylab = "Negative Log Likelihood")
```
By the plots, it can be interpreted that optim likes to blow up the value of A, the amplitude. The larger the value of A, the smaller the Negative Log Likelihood. A similar thing happens for the value of gamma and the inverse happens for the value of alpha. This is not ideal.

As the goal is to try to fit to a probability density function, the amplitude can be expressed in terms of the other parameters gamma and alpha. From this point on the core width gamma is referred to as alpha and the power index alpha is referred to as beta

The equation of the probability density function is as follows:
$$f(x) = \frac{\Gamma(\beta)}{\alpha\sqrt\pi~\Gamma(\beta-\frac{1}{2})} \left[1+\frac{(x-\mu)^2}{\alpha^2}\right]^{-\beta}$$

In order for the function to remain positive, alpha must be greater than 0 and beta must be greater than or equal to 1.5.

```{r}
MOFFATPDF <- function(parameters, x){
  mu <- parameters[[1]]
  alpha <- parameters[[2]]
  beta <- parameters[[3]]
  
  predictedDensity <- (gamma(beta)/(alpha*sqrt(pi)*gamma(beta-0.5)))*(1+((x-mu)/alpha)^2)^-beta
  return(predictedDensity)
}

MOFFAT <- function(mu, alpha, beta, x){
  gamma(beta)/(alpha*sqrt(pi)*gamma(beta-0.5))*(1+((x-mu)/alpha)^2)^-beta
}
```

```{r}
#Function 1
mu1 <- 10
alpha1 <- 1.3
beta1 <- 4
String1 = paste("mu =", mu1, "alpha =", alpha1, "beta =", beta1)

#Function 2
mu2 <- -3
alpha2 <- 0.7
beta2 <- 6
String2 = paste("mu =", mu2, "alpha =", alpha2, "beta =", beta2)

#Function 3
mu3 <- 3.4
alpha3 <- 6.4
beta3 <- 2
String3 = paste("mu =", mu3, "alpha =", alpha3, "beta = ", beta3)


ggplot() +
  stat_function(fun = MOFFAT, n = 1000, args = list(mu = mu1, alpha = alpha1, beta = beta1), aes(colour = String1)) +
  stat_function(fun = MOFFAT, n = 1000, args = list(mu = mu2, alpha = alpha2, beta = beta2), aes(colour = String2)) +
  stat_function(fun = MOFFAT, n = 1000, args = list(mu = mu3, alpha = alpha3, beta = beta3), aes(colour = String3)) +
  xlab("x") + ylab("p(x)") +
  xlim(-20, 20) + ylim(0, NA) +
  scale_colour_manual("Moffat Distributions", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("left"),
    legend.margin = margin(6, 6, 6, 6))
```


The optim function has two requirements. The first is a function that it sets out to minimize. For this I will use the Negative Log Likelihood of the function. The second requirement is the gradient of the function that is being minimized. For this I will take the partial derivative of the Negative Log Likelihood with respect to each parameter. 

To obtain the Negative Log Likelihood.
First I take the Likelihood of the parameters of f(x). $L(\mu, \alpha, \beta) = \prod f(x)$
Next I take the log of the likelihood and apply the log law : $ln\left[ab\right] = ln\left[a\right]+ln\left[b\right]$.
$ln\left[L(\mu, \alpha, \beta)\right] = \sum(ln\left[f(x)\right])$
Taking the negative of this yields the Negative Log Likelihood.

$$NLL = -\sum\ln\left[\frac{\Gamma(\beta)}{\alpha\sqrt\pi~\Gamma(\beta-\frac{1}{2})} \left[1+\frac{(x-\mu)^2}{\alpha^2}\right]^{-\beta}\right]$$

```{r}
NLL <- function(parameters, x){
  predictedDensity <- MOFFATPDF(parameters, x)
  return(-sum(log(predictedDensity)))
}
```

Now I need the partial derivatives of the negative log likelihood with respect to each parameter.
Before doing so I applied some log laws to transform the equation.

Log Laws:
$ln\left[ab\right] = ln\left[a\right]+ln\left[b\right]$
$ln\left[\frac{a}{b}\right] = ln\left[a\right]-ln\left[b\right]$
$ln\left[a\right]^b = b\times\ln\left[a\right]$

By applying these log laws the negative log likelihood transforms into an equation that is simpler to obtain the partial derivatives to. 


$$-\sum(\ln\left[\Gamma(\beta)\right] - (\ln\left[\alpha\sqrt\pi\right] + \ln\left[\Gamma(\beta-\frac{1}{2})\right]) - \beta\ln\left[1+\frac{(x-\mu)^2}{\alpha^2}\right])$$
In order to take the partial derivative with respect to $\mu$ I utilised the chain rule.
Let $ u = \left[1+v^2\alpha^{-2}\right] $.
Let $ v = x - \mu $
$\frac{\delta}{\delta\mu} = \frac{\delta}{\delta u} \frac{\delta u}{\delta v} \frac{\delta v}{\delta \mu}$
$\frac{\delta v}{\delta \mu} = -1$
$\frac{\delta u}{\delta v}  = 2v\alpha^{-2}$
$\frac{\delta}{\delta u} = -\sum(-\frac{\beta}{u}) $

Resulting in
$$\frac{\delta}{\delta\mu} = -\sum\frac{2\beta(x-\mu)}{\alpha^2 + (x-\mu)^2}$$
A similar approach was used to take the partial derivative with respect to $\alpha$ and $\beta$.
Resulting in
$$\frac{\delta}{\delta\alpha} = -\sum-\frac{1}{\alpha} + \frac{2\beta(x-\mu)^2}{\alpha^3 + \alpha(x-\mu)^2}$$
To obtain the partial derivative with respect to beta I will be using the Rcode of digamma
$$\frac{\delta}{\delta\beta} = -\sum digamma(\beta) - digamma(\beta - \frac{1}{2}) - \ln\left[1+\frac{(x-mu)^2}{alpha^2}\right]$$

```{r}
derivMOFFATPDF <- function(parameters, x){
  mu <- parameters[[1]]
  alpha <- parameters[[2]]
  beta <- parameters[[3]]
  
  
  dMu <- -sum((2*beta*(x-mu))/(alpha^2 + (x-mu)^2))
  dAlpha <- -sum(-1/alpha + (2*beta*(x-mu)^2)/(alpha^3+alpha*(x-mu)^2))
  dBeta <- -sum(digamma(beta)-digamma(beta-0.5)-log(1+(x-mu)^2/alpha^2))
  
  return(c(dMu, dAlpha, dBeta))
}
```

Data to optimise.
```{r}
xl2 <- line2$x
xl2.5 <- line2.5$x
xVall2 <- seq(min(xl2), max(xl2), length = 963)
xVall2.5 <- seq(min(xl2.5), max(xl2.5), length = 291)
```

Optimising the parameters.
```{r}
initialGUESS <- c(mu = 4115, alpha = 1, beta = 4)
lowerBOUNDS <- c(0, 1e-6, 1.5)

RESULT <- optim(par = initialGUESS, fn = NLL, gr = derivMOFFATPDF, x = xl2.5, method = "L-BFGS-B", lower = lowerBOUNDS)
optimPAR2.5 <- RESULT$par
optimPAR2.5
```

Visualising the results
```{r}
optimDENS2.5 <- MOFFATPDF(optimPAR2.5, xVall2.5)
## plotting layers in ggplot
ggplot(line2.5) + 
  geom_histogram(aes(x, y=after_stat(density), colour="data"),bins=50,fill=cbPalette[5]) + 
  stat_function(fun=MOFFATPDF,args = list(parameters = optimPAR2.5), n=1000, aes(colour="Moffat PDF"), linewidth=1.2) +
  ylab("p(x)") + 
  scale_colour_manual("Legend", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("left"),
    legend.margin = margin(6, 6, 6, 6))
```


Optimising the parameters on a larger dataset.
```{r}
RESULT <- optim(par = initialGUESS, fn = NLL, gr = derivMOFFATPDF, x = xl2, method = "L-BFGS-B", lower = lowerBOUNDS)
optimPAR2 <- RESULT$par
optimPAR2
```
Visualising the results
```{r}
optimDENS2 <- MOFFATPDF(optimPAR2, xVall2)
## plotting layers in ggplot
ggplot(line2) + 
  geom_histogram(aes(x, y=after_stat(density), colour="data"),bins=50,fill=cbPalette[5]) + 
  stat_function(fun=MOFFATPDF,args = list(parameters = optimPAR2), n=1000, aes(colour="Moffat PDF"), linewidth=1.2) +
  ylab("p(x)") + 
  scale_colour_manual("Legend", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("right"),
    legend.margin = margin(6, 6, 6, 6))
```

Visually comparing the Moffat PDF and the Normal PDF when the parameters are estimated.
```{r}
ggplot(line2) +
  geom_histogram(aes(x, y = after_stat(density), colour = "data"), bins = 50, fill=cbPalette[5]) +
  stat_function(fun = gaussian_density, args = list(parameters = optimized_parameters), n=1000, aes(colour="gaussian density"), linewidth=1.2) +
  stat_function(fun=MOFFATPDF,args = list(parameters = optimPAR2), n=1000, aes(colour="Moffat PDF"), linewidth=1.2) +
  ylab("p(x)") + 
  scale_colour_manual("Legend", values = cbPalette) +
  theme(legend.position = "inside",
    legend.justification = c("right"),
    legend.margin = margin(6, 6, 6, 6))
```

It is clear that the 1D Moffat Distribution provides is a much better fit to the data.

Now I will attempt to fit a 2D Moffat Distribution to the data.

GOODNESS OF FIT 
Kolmogorov-Smirnov Tests

For unknown distribution F.
$F_0$ is a pre-specified, not data dependent distribution model. < Problematic statement in this instance.
As the parameters of the Moffat 1D distribution model were derived from the data.
This will lead to highly conservative tests.

$H_0 : F = F_0$
$H_1 : F \neq F_0$

Test purpose: Given that $X_1, X_2, ..., X_n \sim F$. Test $H_0$ against all alternatives in $H_1$
Statistic Definition: Supremum Distance ($D_n$) between $F_n$ and $F_0$. Where $F_n(x) = \frac{1}{n}\sum^{n}_{i = 1}1_{X_i\leq x}$
$D_n := \sqrt{n}sup|F_n(x) - F_0(x)$
If the null hypothesis holds then the value of $D_n$ tends to be small.

The maximum difference betwen $F_0$ and $F_n$ happens at a certain value of $X_i$
$D_n = max(D_n^+, D_n^-)$
$D_n^+ = \sqrt{n} max(\frac{i}{n} - U_i)$
$D_n^- = \sqrt{n} max(U_i - \frac{i - 1}{n})$
Where $U_j$ is the j-th sorted $U_i := F_0(X_i)$

$D_n$ has an asymptotic cdf given by the Kolmogorov-Smirnov K function:
$K(x) := 1 - 2\sum_{j = 1}^{\infty}(-1)^{j-1}e^{-2j^2x^2}$

Let F be the data in line 2
Let $F_0$ be the modelled normal distribution

```{r}
ks.test(x = line2$x, y = "pnorm")
```

```{r}
noTiesL2 <- unique(xl2)
ks.test(x = noTiesL2, y = "pnorm")
```
Strongly rejects the null hypothesis

```{r}
plot(ecdf(noTiesL2), main = "KS Test - Normal Distribution", ylab = "Probability")
curve(pnorm(x, mean = 4115.19535, sd = 1.20138), add = TRUE, col = 4)
```

Let $F_0$ be the modelled 1D Moffat Distribution.

```{r}
xRange <- seq(4105, 4125, length = 100)
integrate(MOFFATPDF, lower = min(xRange), upper = max(xRange), parameters = optimPAR2)

MOFFATCDF <- vector("numeric", length = 100)

for (i in 1:100) {
  MOFFATCDF[i] <- integrate(MOFFATPDF, lower = xRange[1], upper = xRange[i], parameters = optimPAR2)$value
}
MOFFATCDF
```

```{r}
CDF <- cbind(xRange, MOFFATCDF)

plot(ecdf(noTiesL2), main = "KS Test - Moffat Distribution", ylab = "Probability", col = rgb(0,0,1, alpha = 0.1))
lines(CDF[,1], CDF[,2], type = "l", col = cbPalette[7], lwd = 2)
```

```{r}
ks.test(x = noTiesL2, y = MOFFATCDF)
```
tweak 
```{r}
xRange <- seq(4105, 4125, length = 100)
MCDF <- function(x){

   return(integrate(MOFFATPDF, lower = 4100, upper = x, parameters = optimPAR2)$value)
}

VMCDF <- Vectorize(MCDF, vectorize.args = "x")
VMCDF(x = 4115:4225)
```
```{r}
#ordering the data
sortNTL2 <- sort(noTiesL2)
#sortNTL2
#MCDF(xRange = sortNTL2, pdf = MOFFATPDF)
```


```{r}
ks.test(x = sortNTL2, y = VMCDF)
```

```{r}
plot(ecdf(noTiesL2), main = "KS Test", ylab = "Probability", col = cbPalette[5])
curve(pnorm(x, mean = 4115.19535, sd = 1.20138), add = TRUE, col = cbPalette[3], lwd = 2)
lines(CDF[,1], CDF[,2], type = "l", col = cbPalette[2], lwd = 2)
legend("bottomright", legend = c("ECDF", "Normal Dist  D = 1", "1D Moffat Dist  D = 0.0366"), col = cbPalette[c(5,3,2)], lty = c(1,1,1), lwd = c(1,2,2))
```

Visually and numerically, it is evident that the normal distribution is not a good fit for the data.
While the null hypothesis of the Kolmogorov-Smirnov test is accepted for the 1D Moffat Distribution.

Accept the null hypothesis
!Warning assumption violation - model derived from the dataset. Use bootstrapping KS or Anderson-Darling
```{r}
library(DescTools)
AndersonDarlingTest(x = sortNTL2, null = VMCDF)
AndersonDarlingTest(x = sortNTL2, null = "pnorm")
```
```{r}
library(goftest)
ad.test(x = sortNTL2, null = "norm")
```


The next goal is to create some pp plots and qq plots for the 1D Moffat Function.
in order to do that I require a function that calculates the Quantiles of the moffat function.

The QQ-plot should compare the the quantiles from the data to the quantiles of the 1D Moffat model.
Here is the quantiles from the data.
```{r}
quantile(sortNTL2)
```
Now i need to calculate the quantiles of the create 1D Moffat model.
In order to find the quantiles, I will use interpolation.

```{r}
# Define the inverse cumulative distribution function (quantile function) for Moffat distribution
qmoffat <- function(p, params) {
  # Define a function to find the root (quantile) of CDF - p, this equates to 0.
  findRoot <- function(x) {
    integrate(MOFFATPDF, lower = -Inf, upper = x, parameters = params)$value - p
  }
  
  # Find the root (quantile) using uniroot. For a probability, this will look to find a value of x that causes the findRoot function to equate to 0. This value of x is the quantile.
  quantile <- uniroot(findRoot, interval = c(4000,4200))$root
  return(quantile)
}

probs <- seq(0.01, 0.99, by = 0.01)

qmoffat(0.5, optimPAR2)

Vqmoffat <- Vectorize(qmoffat, vectorize.args = "p")
quantiles <- Vqmoffat(p = probs, params = optimPAR2)

data.frame(probs, quantiles)
```
```{r}
quantile(sortNTL2) #data
quantile(quantiles) #theoretical values
```
```{r}
quant <- qnorm(p = probs, mean = optimized_parameters[1], sd = optimized_parameters[2])
qqplot(quant, sortNTL2, xlab = "Normal Distribution Theoretical Quantiles", ylab = "Data Quantiles")
abline(a = 0, b = 1, col = "red")
```


```{r}
qqplot(quantiles, sortNTL2,  xlab = "Moffat Model Theoretical Quantiles", ylab = "Data Quantiles")
abline(a = 0, b =1, col = "red")
par(pty = "s")
qqplot(quantiles, sortNTL2, ylim = c(4112, 4118), xlab = "Moffat Model Theoretical Quantiles", ylab = "Data Quantiles")
abline(a = 0, b = 1, col = "red")
```

A P-P plot  is used for assessing how close a dataset fits to a particular model. In this instance it is used to see how well the dataset (the "1 Dimensional" Slice of the celestial source) fits to the 1D Moffat distribution with the parameters given by the previous optimisation. 
The two cumulative distribution functions are plotted against each other. 

```{r}
xRANGE <- seq(4105, 4125, length = length(sortNTL2))
model <- VMCDF(xRANGE) #this represents the cumulative distribution of the 1D moffat distribution 
plot(x = xRANGE, y = model)
dataset <- ecdf(sortNTL2)#this represents the cumulative distribution of the dataset
plot(dataset)
```

```{r}
thereoticalProbs <- model
empiricalProbs <- dataset(xRANGE)

plot(thereoticalProbs, empiricalProbs, main = "P-P Plot", xlab = "Theoretical Probabilities", ylab = "Sample Probabilities", col = "blue", pch = 16)
abline(0, 1, col = "red", lty = 2)  # Add a reference line
```
```{r}
par(mfrow = c(1,2))
qqplot(quantiles, sortNTL2,  xlab = "Moffat Model Theoretical Quantiles", ylab = "Data Quantiles")
abline(a = 0, b =1, col = "red")
plot(thereoticalProbs, empiricalProbs, main = "P-P Plot", xlab = "Theoretical Probabilities", ylab = "Sample Probabilities", col = "blue", pch = 16)
abline(0, 1, col = "red", lwd = 2)  # Add a reference line
```


plot with quartiles
```{r}
q <- quantile(quantiles)
ggplot(line2) + 
  geom_histogram(aes(x, y=after_stat(density), colour="data"),bins=50,fill=cbPalette[5]) + 
  stat_function(fun=MOFFATPDF,args = list(parameters = optimPAR2), n=1000, aes(colour="Moffat PDF"), linewidth=1.2) +
  ylab("p(x)") + 
  scale_colour_manual("Legend", values = cbPalette) +
  geom_vline(xintercept = c(q[2], q[3], q[4]), linetype = "dashed", lwd = 0.2, col = "black")

```
