result <- optim(par = initial_guess, fn = negative_log_likelihood, x = sampleData, method = "Nelder-Mead")
# Extract the optimized parameters
optimized_parameters <- result$par
print(optimized_parameters)
#plot time
xValN <- seq(min(sampleData), max(sampleData), length = 1000)
optimisedDensN <- gaussian_density(x = xValN, mean = optimized_parameters[1], sd = optimized_parameters[2])
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
install.packages("caret")
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
install.packages("corrplot")
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
library(ggthemes) # load ggthemes
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
library(ggthemes) # load ggthemes
install.packages("ggthemes")
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
library(ggthemes) # load ggthemes
library(flextable) # load flextable
library(party) # load party
install.packages("party")
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
library(ggthemes) # load ggthemes
library(flextable) # load flextable
library(party) # load party
library(caTools) # load caTools
library(dslabs) # load dslabs
library(Metrics) # load Metrics
install.packages("Metrics")
# Load required packages
library(tidyverse) # load tidyverse
library(caret) # load caret
library(corrplot) # load corrplot
library(ggthemes) # load ggthemes
library(flextable) # load flextable
library(party) # load party
library(caTools) # load caTools
library(dslabs) # load dslabs
library(Metrics) # load Metrics
GCd <- caret::GermanCredit
View(GermanCredit)
data("GermanCredit")
data("GermanCredit")
dim("GermanCredit")
data("GermanCredit")
dim(GermanCredit)
data("GermanCredit")
dim(GermanCredit)
#Ensure that there are no missing data values
!complete.cases(GermanCredit)
GermanCredit <- GermanCredit %>%
dplyr::relocate(Class, .after = last_col())
# 1.1 Remove all the rows containing missing values - that can introduce some bias into the data
creditClean <- na.omit(creditClean)
GermanCredit <- GermanCredit %>%
dplyr::relocate(Class, .after = last_col())
# 1.1 Remove all the rows containing missing values - that can introduce some bias into the data
creditClean <- na.omit(GermanCredit)
# 1.2 Remove all near-zero predictors
nzvPredictors <- caret::nearZeroVar(GermanCredit)
creditClean <- creditClean[,nzvPredictors]
# 1.3 Convert all categorical variables to factors
allVars <- names(creditClean)
nonFactors <- c("Class", "Duration", "Amount",
"InstallmentRatePercentage", "Age")
factorVars <- setdiff(allVars, nonFactors)
creditClean[,factorVars] <- lapply(creditClean[,factorVars], factor)
# 1.4 Check levels() of the Class variable
# Notice anything?
# Display creditClean
creditClean
GermanCredit <- GermanCredit %>%
dplyr::relocate(Class, .after = last_col())
# 1.1 Remove all the rows containing missing values - that can introduce some bias into the data
creditClean <- na.omit(GermanCredit)
# 1.2 Remove all near-zero predictors
nzvPredictors <- caret::nearZeroVar(GermanCredit)
creditClean <- creditClean[,-nzvPredictors]
# 1.3 Convert all categorical variables to factors
allVars <- names(creditClean)
nonFactors <- c("Class", "Duration", "Amount",
"InstallmentRatePercentage", "Age")
factorVars <- setdiff(allVars, nonFactors)
creditClean[,factorVars] <- lapply(creditClean[,factorVars], factor)
# 1.4 Check levels() of the Class variable
# Notice anything?
# Display creditClean
creditClean
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(GermanCredit, prop = 0.80)
install.packages("rsample")
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(GermanCredit, prop = 0.80)
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
# Set random seed
set.seed(4003)
# Create stratified partition on Class variable
inTrainIds <- caret::createDataPartition(creditClean$Class,
p = 0.75,
list=FALSE)
length(inTrainIds)
# Create Training Dataset
trainingData <- creditClean[inTrainIds, ]
# Create Test Dataset
testData <- creditClean[-inTrainIds,]
prop.table(table(trainingData$Class))
prop.table(table(testData$Class))
# Summary trainingData
glimpse(trainingData)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~Duration + Amount + InstallmentRatePercentage + Age + CheckingAccountStatus.none + CreditHistory.Critical + Housing.Own + SavingsAccountBonds.Unknown + Property.Union, data = chosenVars)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~Duration + Amount + InstallmentRatePercentage + Age + CheckingAccountStatus.none + CreditHistory.Critical + Housing.Own + SavingsAccountBonds.Unknown + Property.Union, data = GermanClass)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~Duration + Amount + InstallmentRatePercentage + Age + CheckingAccountStatus.none + CreditHistory.Critical + Housing.Own + SavingsAccountBonds.Unknown + Property.Union, data = GermanCredit)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~Duration + Amount + InstallmentRatePercentage + Age + CheckingAccountStatus.none + CreditHistory.Critical + Housing.Own + SavingsAccountBonds.Unknown + Property.Unknown, data = GermanCredit)
GermanCredit <- GermanCredit %>%
dplyr::relocate(Class, .after = last_col())
# 1.1 Remove all the rows containing missing values - that can introduce some bias into the data
creditClean <- na.omit(GermanCredit)
# 1.2 Remove all near-zero predictors
nzvPredictors <- caret::nearZeroVar(creditClean)
creditClean <- creditClean[,-nzvPredictors]
# 1.3 Convert all categorical variables to factors
allVars <- names(creditClean)
nonFactors <- c("Class", "Duration", "Amount",
"InstallmentRatePercentage", "Age")
factorVars <- setdiff(allVars, nonFactors)
creditClean[,factorVars] <- lapply(creditClean[,factorVars], factor)
# 1.4 Check levels() of the Class variable
# Notice anything?
# Display creditClean
creditClean
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(creditClean, prop = 0.80)
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(creditClean, prop = 0.80)
#ensure that the proportions of good/bad is similar to the total data set
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
trainingData$Class
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(creditClean, prop = 0.80)
#ensure that the proportions of good/bad is similar to the total data set
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
prop.table(table(trainingData$Class))
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(creditClean, prop = 0.80)
#ensure that the proportions of good/bad is similar to the total data set
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
prop.table(table(trainingData$Class))
prop.table(table(testData$Class))
# 2.1 Set random seed for reproducibility
set.seed(4003)
# 2.2 Create initial Train/Test split of creditClean (Train 80% and Test 20%)
initSplit <- rsample::initial_split(creditClean, prop = 0.80)
#ensure that the proportions of good/bad is similar to the total data set
# Create Training and Test datasets
trainingData <- rsample::training(initSplit) # use training() on initSplit
testData <- rsample::testing(initSplit) # use testing() on initSplit
# Check number of records in each split
nrow(trainingData)
nrow(testData)
prop.table(table(trainingData$Class))
prop.table(table(testData$Class))
prop.table(table(creditClean$Class))
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = Binomial(link~"logit") , data = trainingDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = Binomial, link= "logit" , data = trainingDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = "Binomial", link= "logit" , data = trainingDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = "binomial", link= "logit" , data = trainingDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial, link= "logit" , data = trainingDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
logitPred <- predict(..., data = testDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(Class~. family = binmial(link = "logit"), data = testDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(Class~., family = binmial(link = "logit"), data = testDataSmall)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall)
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
predClass <- ifelse(...)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall)
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall, type = "response")
#response gives predictions between 0 and 1
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
predClass <- ifelse(logitPred < 0.5, "Good", "Bad")
table(predClass)
predClass <- predClass
caret::confusionMatrix(predClass)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall, type = "response")
#response gives predictions between 0 and 1
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
predClass <- ifelse(logitPred < 0.5, "Good", "Bad")
table(predClass)
predClass <- predClass
caret::confusionMatrix(predClass)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall, type = "response")
#response gives predictions between 0 and 1
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
predClass <- ifelse(logitPred < 0.5, "Good", "Bad")
table(predClass)
predClass <- predClass
caret::confusionMatrix(factor(predClass), testData$Class)
# Set random seed
set.seed(4003)
# Create stratified partition on Class variable
inTrainIds <- caret::createDataPartition(creditClean$Class,
p = 0.75,
list=FALSE)
length(inTrainIds)
# Create Training Dataset
trainingData <- creditClean[inTrainIds, ]
# Create Test Dataset
testData <- creditClean[-inTrainIds,]
prop.table(table(trainingData$Class))
prop.table(table(testData$Class))
# Summary trainingData
glimpse(trainingData)
# Using a reduced set of the variables available
chosenVars <- c("Class", "Duration", "Amount", "InstallmentRatePercentage",
"Age", "CheckingAccountStatus.none", "CreditHistory.Critical",
"Housing.Own", "SavingsAccountBonds.Unknown", "Property.Unknown")
trainingDataSmall <- trainingData[, chosenVars]
testDataSmall <- testData[, chosenVars]
# Fit a logistic regression to the training data and predict for the test data
logitModel <- glm(Class~ . , family = binomial(link = "logit"), data = trainingDataSmall)
#Skipping diagnostics for now
logitPred <- predict(logitModel, newdata = testDataSmall, type = "response")
#response gives predictions between 0 and 1
# Create a table or confusion matrix of the predicted and actual Class if a cut-off of 0.5 is used
predClass <- ifelse(logitPred < 0.5, "Good", "Bad")
table(predClass)
predClass <- predClass
caret::confusionMatrix(factor(predClass), testData$Class)
rfModel <- caret::train(Class ~ .,
data = trainingData,
method = "ranger")
plot(rfModel)
rfModel
rfModel_mtry10 <- caret::train(Class ~ .,
data = trainingData,
method = "ranger",
tuneLength = 10)
plot(rfModel_mtry10)
################################################
# Specify Tuning Grid for Random Forest        #
################################################
mtryGrid <- data.frame(mtry=seq(5,25,by=2),
splitrule = "gini",
min.node.size = 1)
#################################################
set.seed(4003)
# tuningGrid --- use to tune model.
# WARNING: the following commands take a while!
rfModel_mtry11 <- caret::train(Class ~ .,
data = trainingData,
method = "ranger",
tuneGrid = mtryGrid)
plot(rfModel_mtry11)
creditControl <- trainControl(method = "cv",
number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
verboseIter = TRUE)
enetModel <- caret::train(Class ~ .,
data = trainingData,
method = "glmnet",
trControl = creditControl)
plot(enetModel)
enetGrid <- expand.grid(alpha=0:1,
lambda = seq(0.0001, 0.05, length=50))
enetModel2 <- caret::train(Class ~ .,
data = trainingData,
method = "glmnet",
tuneGrid = enetGrid,
trControl = creditControl)
plot(enetModel2)
plot(enetModel2$finalModel)
set.seed(4300)
cvFolds <- caret::createFolds(trainingData$Class, k=10)
# Share cvFolds inside the common trainControl object
creditControl <- caret::trainControl(index = cvFolds,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = TRUE,
verboseIter = TRUE)
# Fit glmnet using creditControl
enetGrid2 <- expand.grid(alpha=1,
lambda = seq(0.04, 0.06, length=50))
enetModel <- caret::train(Class ~ .,
data = trainingData,
metric = "ROC",
# Specify model
method = "glmnet",
# Use custom tuning grid
tuneGrid = enetGrid2,
# Use shared trainingControl object
trControl = creditControl)
plot(enetModel)
# Fit random forest model using creditControl
rfModel  <- train(Class ~ .,
data = trainingData,
metric = "ROC",
# Specify model
method = "ranger",
# Use tuning grid of length 25
tuneLength = 25,
# Use shared trainingControl object
trControl = creditControl)
# Plot rfModel
plot(rfModel)
################################
modelList <- list(glmnet = enetModel,
rf = rfModel)
resamps <- caret::resamples(modelList)
summary(resamps)
bwplot(resamps, metric = c("ROC", "Sens"))
xyplot(resamps, metric = c("Sens"))
xyplot(resamps, metric = c("ROC"))
creditTree <- ctree(Class ~ ., data = trainingData)
creditTree
plot(creditTree)
predClass <- predict(creditTree,testData)
confusionMatrix(predClass,testData$Class)
# Load gapminder data
data("gapminder")
# Assign gapminder to gapData
gapData <- gapminder
# Remove gapminder from memory
rm(gapminder)
